# Code Structure Documentation

## ðŸ“ **File Organization**

```
kth/anomly-detection/
â”œâ”€â”€ anomaly_detection.ipynb              # Main notebook implementation
â”œâ”€â”€ MVSEC_Anomaly_Detection_Documentation.md  # Comprehensive documentation
â”œâ”€â”€ Code_Structure_Documentation.md      # This file
â”œâ”€â”€ system_architecture_diagram.py       # Diagram generation script
â”œâ”€â”€ data/                               # MVSEC dataset directory
â”‚   â”œâ”€â”€ indoor_flying2_data-002.hdf5   # Event data files
â”‚   â”œâ”€â”€ indoor_flying3_data-003.hdf5
â”‚   â”œâ”€â”€ outdoor_day1_data-008.hdf5
â”‚   â””â”€â”€ ...                            # Additional MVSEC files
â””â”€â”€ output/                            # Generated visualizations
    â”œâ”€â”€ system_architecture.png
    â”œâ”€â”€ data_flow_diagram.png
    â”œâ”€â”€ model_architectures.png
    â”œâ”€â”€ anomaly_strategy.png
    â””â”€â”€ evaluation_framework.png
```

## ðŸ”§ **Notebook Cell Structure**

### **Cell Organization Hierarchy**

```
anomaly_detection.ipynb
â”œâ”€â”€ ðŸ“– [1] Project Overview & Documentation
â”œâ”€â”€ ðŸ”§ [2] Environment Setup & Imports
â”œâ”€â”€ ðŸ’¾ [3] Data Loading Configuration
â”œâ”€â”€ ðŸ”„ [4] Data Pipeline Documentation
â”œâ”€â”€ ðŸ“¥ [5] MVSEC Data Loading Functions
â”œâ”€â”€ ðŸ—ï¸ [6] Data Handler Class Implementation
â”œâ”€â”€ ðŸŽ­ [7] Anomaly Generation Documentation
â”œâ”€â”€ ðŸŽ² [8] Anomaly Generator Implementation
â”œâ”€â”€ ðŸ“¦ [9] Dataset Creation Class
â”œâ”€â”€ ðŸ§  [10] Neural Architecture Documentation
â”œâ”€â”€ âš¡ [11] Spiking Neural Network Implementation
â”œâ”€â”€ ðŸ”„ [12] RNN & TCN Implementation
â”œâ”€â”€ ðŸ‹ï¸ [13] Training & Evaluation Framework
â”œâ”€â”€ ðŸ“Š [14] Visualization Functions
â”œâ”€â”€ ðŸ”— [15] Complete Pipeline Function
â”œâ”€â”€ ðŸš€ [16] Main Execution Pipeline
â”œâ”€â”€ ðŸ§ª [17] Data Loading Test
â””â”€â”€ ðŸ“‹ [18] Results & Conclusion
```

## ðŸ—ï¸ **Class Architecture**

### **Core Classes Hierarchy**

```
Data Processing Layer:
â”œâ”€â”€ MVSECDataHandler
â”‚   â”œâ”€â”€ __init__(data_path, sequence, camera, sensor_size)
â”‚   â”œâ”€â”€ load_data(max_events) â†’ events_dict
â”‚   â”œâ”€â”€ preprocess_events(events, num_frames) â†’ frame_tensor
â”‚   â””â”€â”€ create_dataset(num_frames, frame_size) â†’ processed_frames

Anomaly Generation Layer:
â”œâ”€â”€ AnomalyGenerator
â”‚   â”œâ”€â”€ __init__(seed)
â”‚   â”œâ”€â”€ add_blackout_region(frame, region_size, intensity)
â”‚   â”œâ”€â”€ add_vibration_noise(frame, region_size, intensity)
â”‚   â”œâ”€â”€ flip_polarities(frame, region_size, flip_prob)
â”‚   â””â”€â”€ add_random_anomaly(frame, anomaly_type)

Dataset Layer:
â”œâ”€â”€ EventAnomalyDataset(Dataset)
â”‚   â”œâ”€â”€ __init__(frames, anomaly_ratio, transform)
â”‚   â”œâ”€â”€ __len__() â†’ int
â”‚   â”œâ”€â”€ __getitem__(idx) â†’ (frame, label, mask, anomaly_type)
â”‚   â””â”€â”€ [Pre-generated anomaly storage for efficiency]

Model Layer:
â”œâ”€â”€ SpikingAnomalyDetector(nn.Module)
â”‚   â”œâ”€â”€ SpikingConv2d layers (3x)
â”‚   â”œâ”€â”€ Global average pooling
â”‚   â”œâ”€â”€ Linear classification head
â”‚   â””â”€â”€ reset_membrane_potentials()
â”‚
â”œâ”€â”€ RNNAnomalyDetector(nn.Module)
â”‚   â”œâ”€â”€ Conv2d feature extraction (2x)
â”‚   â”œâ”€â”€ GRU temporal processing
â”‚   â””â”€â”€ Linear classification head
â”‚
â””â”€â”€ TCNAnomalyDetector(nn.Module)
    â”œâ”€â”€ TemporalBlock layers (3x)
    â”œâ”€â”€ Global average pooling
    â””â”€â”€ Linear classification head
```

## ðŸ”„ **Function Flow Diagram**

### **Main Pipeline Execution Flow**

```mermaid
graph TD
    A[load_mvsec_data] --> B[MVSECDataHandler.create_dataset]
    B --> C[EventAnomalyDataset.__init__]
    C --> D[random_split]
    D --> E[DataLoader creation]
    E --> F[Model instantiation]
    F --> G[train_model]
    G --> H[test_model]
    H --> I[Visualization & Results]

    B --> B1[load_data]
    B --> B2[preprocess_events]
    B --> B3[Frame resizing]

    C --> C1[AnomalyGenerator.add_random_anomaly]
    C1 --> C2[Pre-generate all anomalies]

    G --> G1[train_one_epoch loop]
    G1 --> G2[evaluate_model]
    G2 --> G3[Learning rate scheduling]
```

## ðŸ“Š **Data Flow Architecture**

### **Data Transformation Pipeline**

```
Raw MVSEC HDF5 Files
â””â”€â”€ Format: davis/left/events â†’ [x, y, timestamp, polarity] arrays

Event Dictionary Extraction
â””â”€â”€ Structure: {'x': array, 'y': array, 't': array, 'p': array}

Temporal Binning Process
â”œâ”€â”€ Time range division: t_min â†’ t_max / num_frames
â”œâ”€â”€ Spatial mapping: (x,y) coordinates â†’ pixel locations
â”œâ”€â”€ Channel separation: polarity (+1/-1) â†’ channels (0/1)
â””â”€â”€ Normalization: raw counts â†’ [0, 1] intensity range

Frame Tensor Generation
â””â”€â”€ Output shape: (num_frames, 2, height, width)
    â”œâ”€â”€ Dimension 0: Temporal sequence (50 frames)
    â”œâ”€â”€ Dimension 1: Polarity channels (pos/neg events)
    â”œâ”€â”€ Dimension 2: Spatial height (64 pixels)
    â””â”€â”€ Dimension 3: Spatial width (64 pixels)

Anomaly Injection Process
â”œâ”€â”€ Random selection: 50% of frames marked for anomalies
â”œâ”€â”€ Anomaly type selection: Uniform distribution across 3 types
â”œâ”€â”€ Parameter randomization: Region size, intensity, position
â””â”€â”€ Mask generation: Binary masks for anomaly localization

Final Dataset Structure
â””â”€â”€ PyTorch Dataset: (frame_tensor, binary_label, anomaly_mask, anomaly_type)
```

## ðŸ§® **Algorithm Implementation Details**

### **Spiking Neural Network Algorithm**

```python
# Membrane Potential Dynamics
V[t] = Î² * V[t-1] + I[t]              # Leaky integration
S[t] = Heaviside(V[t] - Î¸)            # Spike generation
V[t] = V[t] - S[t] * Î¸                # Reset after spike

# Surrogate Gradient for Backpropagation
âˆ‚S/âˆ‚V = Î± * exp(-Î±|V-Î¸|) / (1 + exp(-Î±(V-Î¸)))Â²

# Where:
# Î² = membrane decay factor (0.9)
# Î¸ = firing threshold (1.0)
# Î± = surrogate gradient steepness (10.0)
# I[t] = input current from previous layer
```

### **Temporal Binning Algorithm**

```python
# Event-to-Frame Conversion Process
for each event (x, y, t, p):
    bin_index = floor((t - t_min) / bin_width)
    channel = 0 if p == +1 else 1
    frame[bin_index, channel, y, x] += 1

# Normalization per frame and channel
for frame_idx in range(num_frames):
    for channel in range(2):
        max_val = frame[frame_idx, channel].max()
        if max_val > 0:
            frame[frame_idx, channel] /= max_val
```

### **Anomaly Injection Algorithm**

```python
# Systematic Anomaly Generation
def generate_balanced_dataset(frames, anomaly_ratio=0.5):
    num_anomalies = int(len(frames) * anomaly_ratio)
    anomaly_indices = random.choice(len(frames), num_anomalies)

    for idx in anomaly_indices:
        anomaly_type = random.choice(['blackout', 'vibration', 'flip'])
        frame[idx], mask[idx] = apply_anomaly(frame[idx], anomaly_type)

    return labeled_dataset
```

## ðŸŽ¯ **Performance Optimization Strategies**

### **Memory Management**

1. **Event Sampling**: Limit to 500K events per sequence
   ```python
   if total_events > max_events:
       indices = np.linspace(0, total_events-1, max_events, dtype=int)
       events = {key: events[key][indices] for key in events}
   ```

2. **Batch Processing**: Small batch sizes (8-16) for memory efficiency
   ```python
   train_loader = DataLoader(dataset, batch_size=8, shuffle=True)
   ```

3. **Pre-computation**: Generate all anomalies during dataset creation
   ```python
   # Pre-generate anomalies in __init__ rather than __getitem__
   self.anomaly_frames = [self.generate_anomaly(frame) for frame in frames]
   ```

### **Computational Efficiency**

1. **Spatial Downsampling**: 260Ã—346 â†’ 64Ã—64 resolution
2. **Temporal Compression**: ~25M events â†’ 50 frame sequence
3. **Channel Optimization**: Separate pos/neg processing pipelines
4. **Model Simplification**: Focused architectures with minimal parameters

### **Training Optimization**

1. **Learning Rate Scheduling**: ReduceLROnPlateau for convergence
2. **Early Stopping**: Validation loss monitoring
3. **Gradient Clipping**: Prevent exploding gradients in SNN
4. **Memory Reset**: Clear SNN membrane states between batches

## ðŸ” **Error Handling & Robustness**

### **Data Loading Robustness**

```python
try:
    events, sensor_size = load_mvsec_data(data_path, sequence, camera)
except FileNotFoundError:
    raise ValueError(f"MVSEC data not found in {data_path}")
except KeyError as e:
    raise ValueError(f"Invalid HDF5 structure: missing {e}")
except Exception as e:
    logger.error(f"Unexpected error loading data: {e}")
    raise
```

### **Training Stability**

```python
# SNN-specific stability measures
if isinstance(model, SpikingAnomalyDetector):
    loss.backward(retain_graph=True)  # Handle graph connectivity
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
else:
    loss.backward()
```

### **Memory Management**

```python
# Explicit cleanup for large datasets
del events, frames  # Clear intermediate variables
torch.cuda.empty_cache()  # GPU memory cleanup
gc.collect()  # Python garbage collection
```

## ðŸ“ˆ **Extensibility & Modularity**

### **Adding New Anomaly Types**

```python
class AnomalyGenerator:
    def add_custom_anomaly(self, frame, **params):
        """Template for new anomaly types"""
        modified_frame = frame.clone()
        anomaly_mask = torch.zeros_like(frame[0], dtype=torch.bool)

        # Custom anomaly implementation here

        return modified_frame, anomaly_mask

    def register_anomaly_type(self, name, function):
        """Dynamic anomaly registration"""
        self.anomaly_types[name] = function
```

### **Adding New Model Architectures**

```python
class NewArchitectureDetector(nn.Module):
    """Template for additional neural architectures"""
    def __init__(self, input_channels, **kwargs):
        super().__init__()
        # Architecture-specific initialization

    def forward(self, x):
        # Architecture-specific forward pass
        return output
```

### **Configuration Management**

```python
# Centralized configuration dictionary
CONFIG = {
    'data': {
        'sequence': 'indoor_flying',
        'camera': 'left',
        'max_events': 500000,
        'sensor_size': (64, 64),
        'num_frames': 50
    },
    'training': {
        'batch_size': 8,
        'num_epochs': 10,
        'learning_rate': 0.001,
        'anomaly_ratio': 0.5
    },
    'models': {
        'snn': {'beta': 0.9, 'threshold': 1.0},
        'rnn': {'hidden_dim': 64},
        'tcn': {'hidden_channels': [16, 32, 64]}
    }
}
```

## ðŸ§ª **Testing & Validation Framework**

### **Unit Testing Structure**

```python
def test_data_loading():
    """Test MVSEC data loading functionality"""
    events, sensor_size = load_mvsec_data('./test_data', 'indoor_flying', 'left')
    assert len(events['x']) > 0
    assert sensor_size == (260, 346)

def test_anomaly_generation():
    """Test anomaly injection methods"""
    frame = torch.rand(2, 64, 64)
    anomaly_frame, mask = add_blackout_region(frame, (20, 20))
    assert not torch.equal(frame, anomaly_frame)
    assert mask.sum() > 0

def test_model_forward_pass():
    """Test model forward propagation"""
    model = SpikingAnomalyDetector(input_channels=2)
    input_tensor = torch.rand(4, 2, 64, 64)
    output = model(input_tensor)
    assert output.shape == (4, 2)
```

### **Integration Testing**

```python
def test_end_to_end_pipeline():
    """Test complete pipeline execution"""
    results = run_mvsec_anomaly_detection_pipeline(
        data_path='./test_data',
        num_epochs=1,
        num_frames=5
    )
    assert results is not None
    assert 'models' in results
    assert 'metrics' in results
```

This comprehensive code structure documentation provides a complete blueprint for understanding, extending, and maintaining the anomaly detection system.
